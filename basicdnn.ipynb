{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQbUc8RR24mX",
        "outputId": "543fd529-48ec-4fbe-81e4-09b1abb98219"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326a4f57-a3d5-4501-b82b-146051a2025d"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some Notes on our naive model\n",
        "\n",
        "- We are going to write a network based on what we have learnt so far.\n",
        "- The size of the input image is 28x28x1. We are going to add as many layers as required to reach RF = 32 \"atleast\". "
      ],
      "metadata": {
        "id": "NqQG96j73She"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Class architecture\n",
        "\n",
        "**FirstDNN Class:**\n",
        "\n",
        "The FirstDNN class is a neural network model implemented using the PyTorch library. It consists of several convolutional and pooling layers that perform image classification tasks. This documentation provides an overview of the class and its functionality.\n",
        "\n",
        "**Class Overview:**\n",
        "\n",
        "The FirstDNN class is derived from the nn.Module class, which is the base class for all neural network modules in PyTorch. By inheriting from nn.Module, FirstDNN gains access to convenient methods and functionalities for constructing and training neural networks.\n",
        "\n",
        "**Class Constructor**:\n",
        "\n",
        "- The constructor __init__ initializes the FirstDNN object. It sets up the architecture of the neural network by defining its layers and their configurations. Here are the layers included in the FirstDNN class:\n",
        "\n",
        "- self.conv1: The first convolutional layer with 1 input channel, 32 output channels, and a kernel size of 3x3.\n",
        "- self.conv2: The second convolutional layer with 32 input channels, 64 output channels, and a kernel size of 3x3.\n",
        "- self.pool1: The first max pooling layer with a kernel size of 2x2 and a stride of 2.\n",
        "- self.conv3: The third convolutional layer with 64 input channels, 128 output channels, and a kernel size of 3x3.\n",
        "- self.conv4: The fourth convolutional layer with 128 input channels, 256 output channels, and a kernel size of 3x3.\n",
        "- self.pool2: The second max pooling layer with a kernel size of 2x2 and a stride of 2.\n",
        "- self.conv5: The fifth convolutional layer with 256 input channels, 512 output channels, and a kernel size of 3x3.\n",
        "- self.conv6: The sixth convolutional layer with 512 input channels, 1024 output channels, and a kernel size of 3x3.\n",
        "- self.conv7: The seventh convolutional layer with 1024 input channels, 10 output channels, and a kernel size of 3x3.\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        "- The forward method defines the forward pass of the FirstDNN network. It specifies how input data flows through the defined layers to produce an output. Here is the sequence of operations performed in the forward pass:\n",
        "\n",
        "- The input x is passed through the first convolutional layer (conv1), followed by a ReLU activation function.\n",
        "- The result is then passed through the second convolutional layer (conv2), followed by another ReLU activation function.\n",
        "- The output is fed into the first max pooling layer (pool1).\n",
        "- The output from the pooling layer is passed through the third and fourth convolutional layers (conv3 and conv4), each followed by a ReLU activation function.\n",
        "- The output is fed into the second max pooling layer (pool2).\n",
        "- The output from the pooling layer is passed through the fifth and sixth convolutional layers (conv5 and conv6), each followed by a ReLU activation function.\n",
        "- The output from the sixth convolutional layer is passed through the seventh convolutional layer (conv7) without an activation function.\n",
        "- The output is then flattened using x.view(-1, 10) to reshape it into a 2D tensor.\n",
        "- Finally, a log softmax activation function is applied to the flattened output, and the result is returned."
      ],
      "metadata": {
        "id": "WwcwsuGh7Lwj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "source": [
        "class FirstDNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FirstDNN, self).__init__() #con2d parm ---> input channel, output channel, kernel size\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input-28*28*1 ,RF-3*3*1*32,Output-28*28*32\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input-28*28*32 ,RF-3*3*32*64,Output-28*28*64\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) #input-28*28*64 ,Output-14*14*64\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1) #input-14*14*64 ,RF-3*3*64*128,Output-14*14*128\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) #input-14*14*128 ,RF-3*3*128*256,Output-14*14*256\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) #input-14*14*256 ,Output-7*7*256\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3) #input-7*7*256 ,RF-3*3*254*512,Output-5*5*512\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3) #input-5*5*512 ,RF-3*3*512*1024,Output-3*3*1024\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3) #input-3*3*1024 ,RF-3*3*1024*10,Output-1*1*10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x)))))\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Initialization and Summary\n",
        "The following code demonstrates the initialization of a neural network model and the generation of its summary using the torchsummary library. This documentation will outline the purpose and functionality of the code.\n",
        "\n",
        "- Checking for GPU Availability\n",
        "  - The variable use_cuda is assigned the value of torch.cuda.is_available(). This function checks if a GPU is available for computation. If a GPU is present, use_cuda is set to True; otherwise, it is set to False. This information is used to determine the device on which the model will be trained and run.\n",
        "\n",
        "- Device Selection\n",
        "  - The device variable is initialized based on the value of use_cuda. If use_cuda is True, indicating the presence of a GPU, device is set to \"cuda\" to utilize the GPU for computation. If use_cuda is False, indicating no GPU availability, device is set to \"cpu\", indicating the CPU will be used for computation. The device selection ensures the model runs on the available hardware.\n",
        "\n",
        "- Model Initialization\n",
        "  - An instance of the FirstDNN class is created and assigned to the model variable. This class represents a neural network model implemented using the PyTorch library. The initialization of model does not require any input arguments. By default, the model will be initialized on the CPU.\n",
        "\n",
        "To utilize the chosen device (GPU or CPU), the to(device) method is called on the model. This moves the model's parameters and buffers to the specified device, allowing for computations on that device. If a GPU is available (use_cuda is True), the model will be transferred to the GPU. Otherwise, it remains on the CPU.\n",
        "\n",
        "**Model Summary Generation:**\n",
        "\n",
        "- The summary function from the torchsummary library is used to generate a summary of the model's architecture and parameter information. The summary function takes two arguments: the model instance and the input_size tuple, representing the expected size of the input to the model.\n",
        "\n",
        "- In this case, the input_size is set to (1, 28, 28), indicating that the model expects input tensors with a shape of (batch_size, channels, height, width), where batch_size is flexible, and channels, height, and width are fixed at 1, 28, and 28, respectively. Providing the input_size allows the summary function to calculate the number of parameters in the model and display a summary table with detailed information about each layer, including the output shape and number of parameters."
      ],
      "metadata": {
        "id": "k9aauP_78Gax"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdydjYTZFyi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f879c24-f51e-486b-97d5-68aab74734d2"
      },
      "source": [
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = FirstDNN().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-7bc1a233e240>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zb7l67z823ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe7f8be-4c9a-48f4-acea-1b441f83afc7"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 108831527.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 44226248.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25425977.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 24581327.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Function Documentation\n",
        "\n",
        "\n",
        "### Parameters\n",
        "*   model (nn.Module): The neural network model to be trained.\n",
        "*   device (torch.device): The device (CPU or GPU) on which the training will be performed.\n",
        "*   train_loader (torch.utils.data.DataLoader): The data loader object that provides the training dataset in batches.\n",
        "*   optimizer (torch.optim.Optimizer): The optimizer used to update the model's parameters.\n",
        "*   epoch (int): The current epoch number.\n",
        "\n",
        "### Function Description\n",
        "The train function performs the training process for a given neural network model. It iterates over the training data in batches and updates the model's parameters based on the calculated loss.\n",
        "\n",
        "### Function Steps\n",
        "*   Set the model in training mode using model.train(). This ensures that the model is prepared for training and enables features such as dropout.\n",
        "*   Create a progress bar (pbar) using tqdm(train_loader) to track the training progress.\n",
        "*   Iterate over the batches of the training data using enumerate(train_loader).\n",
        "*   Retrieve the input data (data) and corresponding target labels (target) from the current batch. Move both data and target to the specified device using data.to(device) and target.to(device).\n",
        "*   Clear the gradients of the optimizer using optimizer.zero_grad() to prepare for a new gradient calculation.\n",
        "*   Perform a forward pass of the input data through the model to obtain the predicted output using output = model(data).\n",
        "*   Calculate the loss between the predicted output and the target labels using the negative log-likelihood loss (F.nll_loss) and assign it to loss.\n",
        "*   Perform backpropagation by calling loss.backward() to compute the gradients of the model's parameters with respect to the loss.\n",
        "*   Update the model's parameters using the optimizer by calling optimizer.step().\n",
        "*   Update the progress bar's description with the current loss and batch index using pbar.set_description(desc=f'loss={loss.item()} batch_id={batch_idx}'). *   This provides real-time feedback on the training progress.\n",
        "*   Repeat steps 4-10 for each batch in the training data.\n",
        "\n",
        "**The training function completes when all batches have been processed for the given.**"
      ],
      "metadata": {
        "id": "xK__mO6v8s1c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "source": [
        "from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Function Documentation\n",
        "The provided code snippet presents a testing function for a trained neural network model. This documentation will describe the purpose and functionality of the function.\n",
        "\n",
        "### Parameters\n",
        "- model (nn.Module): The trained neural network model to be evaluated.\n",
        "- device (torch.device): The device (CPU or GPU) on which the testing will be performed.\n",
        "- test_loader (torch.utils.data.DataLoader): The data loader object that provides the test dataset in batches.\n",
        "\n",
        "### Function Description\n",
        "- The test function evaluates the performance of a trained neural network model by testing it on a separate test dataset. It calculates the average loss and accuracy of the model's predictions.\n",
        "\n",
        "### Function Steps\n",
        "- Set the model in evaluation mode using model.eval(). This ensures that the model is prepared for evaluation and disables features such as dropout.\n",
        "- Initialize variables test_loss and correct to track the cumulative loss and the number of correctly predicted samples, respectively.\n",
        "- Enter a context where gradients are not computed using torch.no_grad().\n",
        "- Iterate over the batches of the test data using a for loop with data and target as loop variables.\n",
        "- Move the input data (data) and target labels (target) to the specified device using data.to(device) and target.to(device).\n",
        "- Perform a forward pass of the input data through the model to obtain the predicted output using output = model(data).\n",
        "- Calculate the loss between the predicted output and the target labels using the negative log-likelihood loss (F.nll_loss) with the reduction set to 'sum'. - Add the batch loss to test_loss.\n",
        "- Find the predicted class labels (pred) by taking the index of the maximum log-probability in each output using output.argmax(dim=1, keepdim=True).\n",
        "- Compare the predicted class labels with the target labels to count the number of correct predictions. Increment correct by the sum of matches using pred.eq(target.view_as(pred)).sum().item().\n",
        "- Repeat steps 4-9 for each batch in the test data.\n",
        "- Calculate the average test loss by dividing test_loss by the total number of samples in the test dataset: test_loss /= len(test_loader.dataset).\n",
        "- Print the test results, including the average loss and accuracy, in a formatted message."
      ],
      "metadata": {
        "id": "yfzuOPsP9ebW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "cRa_bm3L9VnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8W64LvyZ99TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentation result"
      ],
      "metadata": {
        "id": "B7aoSfF899Zd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f6dcad-f680-49b8-a347-0e23dad33114"
      },
      "source": [
        "## Used the given LR and it seems that accuracy is 59%\n",
        "model = FirstDNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-5-7bc1a233e240>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=1.9943186044692993 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9707, Accuracy: 2790/10000 (28%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=1.2586653232574463 batch_id=468: 100%|██████████| 469/469 [00:21<00:00, 21.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1984, Accuracy: 5882/10000 (59%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## increased LR by 10 times and model is stuck which means it is not able to learn.\n",
        "model = FirstDNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrh0XhEf4mgz",
        "outputId": "c1610be7-2fb7-473e-9deb-a6e8a7419c92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-5-7bc1a233e240>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=2.3025858402252197 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 20.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 980/10000 (10%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=2.3025858402252197 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 20.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.3026, Accuracy: 980/10000 (10%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reduced learning rate hee and accuracy seems to have increased\n",
        "model = FirstDNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QrmMQii46N1",
        "outputId": "1fccad94-e9f6-4440-c196-d8da533f6aca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-5-7bc1a233e240>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=0.9290955662727356 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 20.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.1030, Accuracy: 6136/10000 (61%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=0.7543477416038513 batch_id=468: 100%|██████████| 469/469 [00:24<00:00, 18.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.8887, Accuracy: 6575/10000 (66%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## reduced learning rate here and have removed momentum also\n",
        "model = FirstDNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yU0p2p96kFz",
        "outputId": "2b15fc82-b6a8-442a-a1f2-1fcec57c0468"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-5-7bc1a233e240>:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n",
            "loss=2.2996745109558105 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 21.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2983, Accuracy: 3217/10000 (32%)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss=2.2969436645507812 batch_id=468: 100%|██████████| 469/469 [00:22<00:00, 21.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2929, Accuracy: 3855/10000 (39%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
